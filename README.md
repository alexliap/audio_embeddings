# Audio Embeddings

# Table of Contents
1. [Data](#data)
2. [Clustering](#clustering)
3. [Embedding Creation](#embedding-creation)
    1. [Evaluation](#evaluation)
    2. [Train your Autoencoder](#train-your-own-autoencoder)
    3. [Compute Distance](#compute-distance)

This is a repository dedicated to experimentation on song similairity modeling based on pre-extrcacted features. Two routes are explored, one using traditional ML clustering that employs the K-Means algorithm and another that employs DL techinques for embedding creation.

## Data

Detailed instructions on obtaining the dataset used in this project can be found in the [Github Repository](https://github.com/MTG/da-tacos). The dataset consists of two subsets: the benchmark subset and the cover analysis subset, containing pre-extracted features and metadata for 15,000 and 10,000 songs, respectively.

To construct the datasets, specific aggregations were applied. The pre-extracted features for each track were originally in sequential form. In general, when working with audio data, values are represented as sequences whose length is determined by the duration of the audio in seconds multiplied by the sample rate. This results in large arrays that are challenging to manage. To address this, a `time window` and a `hop length` were defined allowing features to be computed at each `time window`, which advances in increments of `hop length` time steps.

Ultimately, the techniques employed in this study do not process sequential data directly. Therefore, two tabular datasets were generated by aggregating the sequence-based features. The script responsible for this transformation can be found in the [build_tabular](https://github.com/alexliap/audio_embeddings/blob/master/cli/build_tabular.py) script.

## Clustering

To gain a comprehensive understanding of the complexity of the problem, we first apply a traditional machine learning algorithm commonly used for clustering: K-Means. This algorithm, though relatively simple, relies on Euclidean distance to determine centroid coordinates. However, one of its main challenges is identifying the optimal number of clusters.

If the cliques within the cover analysis subset were perfectly separable, the ideal number of clusters would be 5,000, corresponding to the number of cliques in the subset. However, in practice, increasing the number of clusters results in poorer clustering performance. Further details can be found in the corresponding [notebook](https://github.com/alexliap/audio_embeddings/blob/master/ml_clustering.ipynb).

Overall, the complexity of the problem exceeds the capabilities of a basic clustering algorithm like K-Means. Consequently, we proceed with a more advanced solution.

## Embedding Creation

A more advanced method for capturing similarity between vectors involves generating embeddings from the original vector. This requires training an Autoencoder deep learning model to reconstruct the original vector after encoding it into a latent representation.

The source code for implementing the Autoencoder can be found [here](https://github.com/alexliap/audio_embeddings/tree/master/src/audio_embeddings). The model used in this experiment is a 207K-parameter Autoencoder with the following layer sizes:

- Encoder: 52 - 300 - 200 - 100 - 50 - 20
- Decoder: 20 - 50 - 100 - 200 - 300 - 52

where 52 represents the number of features for each performance or track. The objective is to learn a latent representation of the tracks, which can then be used for comparison.

Other important configuration decisions:

- Loss Function: Mean Squared Error (MSE) to measure reconstruction accuracy
- Optimizer: Adam

### Evaluation

The model was trained on the cover analysis subset and validated on the benchmark subset. Since this task falls within the domain of unsupervised learning, evaluating the effectiveness of the solution presents a challenge.

To assess performance, we propose measuring the average distance between performances within the same clique and comparing it to the average distance between clique performances and the rest of the dataset. The Euclidean distance metric is used for this comparison.

The resulting distance distributions can be viewed [here](https://github.com/alexliap/audio_embeddings/blob/master/pics/in_clique_vs_out_clique_dists.jpeg). A significant overlap between the two distributions is observed, which may be attributed to several factors:

- The tracks are inherently similar, making separation difficult.
- The original vector representing each track is derived from aggregated features, leading to a loss of important information.
- The Autoencoder model may lack sufficient capacity to effectively distinguish between tracks.

![in_clique_vs_out_clique_dists](https://github.com/alexliap/audio_embeddings/blob/master/pics/in_clique_vs_out_clique_dists.jpeg)

As an alternative approach, a Convolutional Neural Network (CNN)-based Autoencoder could be developed to reconstruct the trackâ€™s spectrogram. In deep learning applications involving audio, CNNs are commonly used due to their effectiveness in feature extraction.

### Train your own Autoencoder

In order to train your own Autoencoder you have to:

- Clone the repository

```bash
git clone https://github.com/alexliap/audio_embeddings.git
```

- Create a virtual environment using Python 3.12 and install the dependencies with

```bash
pip install .
```

- You can run the script `cli/train.py` that uses some default parameteres for the whole process or change them yourself.

```bash
python cli/train.py
```

At the end of the training, the Autoencoder model will be saved at the directory `lightning_logs/model/epoch=X-step=Y.ckpt`.

The model can be loaded by running

```python
from audio_embeddings import AutoEncoder, AutoEncoderModel, Decoder, Encoder

layer_sizes = <layer_list_used_for_trainings>

enc = Encoder(layer_sizes)
dec = Decoder(layer_sizes[::-1])

module = AutoEncoder(encoder=enc, decoder=dec)

model = AutoEncoderModel(autoencoder=module)

model.load_state_dict(
    state_dict=torch.load(
        "lightning_logs/model/epoch=X-step=Y.ckpt", weights_only=True
    )["state_dict"]
)
```

### Compute Distance

In order to compute the distance between 2 feature vectors follow the instruction below.

- Having trained your model you must load it like above.

- Load also 2 feature vectors from the benchmark dataset. You can also load more than 1 for the comparison

```python
import polars as pl

bench = pl.read_csv("tabular_data/benchmark_tabular.csv")

a = bench.sort('work').drop(['work', 'performance']).to_numpy().astype(np.float32)[1, :].reshape(1, -1)
b = bench.sort('work').drop(['work', 'performance']).to_numpy().astype(np.float32)[400, :].reshape(1, -1)

# or
b = bench.sort('work').drop(['work', 'performance']).to_numpy().astype(np.float32)[400:800, :]
```

- Get the euclidean distance between the 2 or more vectors

```python
model.get_distance(a, b)
```
